{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HeyHooooooo/Python/blob/main/Untitled2_ipynb%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bp08z9ZAOW1C"
      },
      "outputs": [],
      "source": [
        "service_key = \"iqwmWSnw3kZXha6%2FhKPJaVimeDjLtAk4pk1u9R%2BjzUOYtzRluxhzvATsMOT5kH505WpusMdouqZLlFXguwqzRA%3D%3D\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYl6q38UP6K0"
      },
      "outputs": [],
      "source": [
        "!pip install PublicDataReader --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfrWHuFQXli3"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcPxS_meP9rE"
      },
      "outputs": [],
      "source": [
        "from PublicDataReader import TransactionPrice\n",
        "api = TransactionPrice(service_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFeXwkOHQL7v"
      },
      "outputs": [],
      "source": [
        "df = api.get_data(\n",
        "    property_type=\"아파트\",\n",
        "    trade_type=\"매매\",\n",
        "    sigungu_code=\"11290\",\n",
        "    start_year_month=\"202302\",\n",
        "    end_year_month=\"202312\",\n",
        "    )\n",
        "df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhL9AsSaWBwA"
      },
      "outputs": [],
      "source": [
        "df.to_csv('table1.csv', encoding='utf-8-sig')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "MtTVx6DrxmeP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Du1G6WI1DIDE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import requests\n",
        "import json\n",
        "import time\n",
        "import dateutil\n",
        "import datetime\n",
        "from dateutil.relativedelta import relativedelta\n",
        "import urllib3\n",
        "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djpjMq5PEkgQ"
      },
      "outputs": [],
      "source": [
        "API_KEY = 'jWRXgXw3mVumPJcsRhPsIXXkIQBlwHIX'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azChvbl6_t32"
      },
      "outputs": [],
      "source": [
        "def send_request(date):\n",
        "    base_url = 'https://api.nytimes.com/svc/archive/v1/'\n",
        "    url = base_url + '/' + date[0] + '/' + date[1] + '.json?api-key=' + API_KEY\n",
        "    try:\n",
        "        response = requests.get(url, verify=False).json()\n",
        "    except Exception:\n",
        "        return None\n",
        "    time.sleep(6)\n",
        "    return response\n",
        "\n",
        "\n",
        "def is_valid(article, date):\n",
        "    is_in_range = date > start and date < end\n",
        "    has_headline = type(article['headline']) == dict and 'main' in article['headline'].keys()\n",
        "    return is_in_range and has_headline\n",
        "\n",
        "\n",
        "def parse_response(response):\n",
        "    data = {'headline': [],\n",
        "        'date': [],\n",
        "        'web_url': [],\n",
        "        'doc_type': [],\n",
        "        'lead_paragraph': [],\n",
        "        'material_type': [],\n",
        "        'author': [],\n",
        "        'section': [],\n",
        "        'subsection': [],\n",
        "        'keywords': []}\n",
        "\n",
        "    articles = response['response']['docs']\n",
        "    for article in articles:\n",
        "        date = dateutil.parser.parse(article['pub_date']).date()\n",
        "        if is_valid(article, date):\n",
        "            data['date'].append(date)\n",
        "            data['headline'].append(article['headline']['main'])\n",
        "            if 'section_name' in article:\n",
        "                data['section'].append(article['section_name'])\n",
        "            else:\n",
        "                data['section'].append(None)\n",
        "            if 'lead_paragraph' in article:\n",
        "                data['lead_paragraph'].append(article['lead_paragraph'])\n",
        "            else:\n",
        "                data['lead_paragraph'].append(None)\n",
        "            if 'web_url' in article:\n",
        "                data['web_url'].append(article['web_url'])\n",
        "            else:\n",
        "                data['web_url'].append(None)\n",
        "            if 'subsection_name' in article:\n",
        "                data['subsection'].append(article['subsection_name'])\n",
        "            else:\n",
        "                data['subsection'].append(None)\n",
        "            if 'byline' in article:\n",
        "                data['author'].append(article['byline']['original'])\n",
        "            else:\n",
        "                data['author'].append(None)\n",
        "            data['doc_type'].append(article['document_type'])\n",
        "            if 'type_of_material' in article:\n",
        "                data['material_type'].append(article['type_of_material'])\n",
        "            else:\n",
        "                data['material_type'].append(None)\n",
        "            keywords = [keyword['value'] for keyword in article['keywords'] if keyword['name'] == 'subject']\n",
        "            data['keywords'].append(keywords)\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "\n",
        "def get_data(dates):\n",
        "    total = 0\n",
        "    print('Date range: ' + str(dates[0]) + ' to ' + str(dates[-1]))\n",
        "    if not os.path.exists('headlines'):\n",
        "        os.mkdir('headlines')\n",
        "    for date in dates:\n",
        "        print('Working on ' + str(date) + '...')\n",
        "        csv_path = 'headlines/' + date[0] + '-' + date[1] + '.csv'\n",
        "        if not os.path.exists(csv_path): # If we don't already have this month\n",
        "            response = send_request(date)\n",
        "            if response is not None:\n",
        "                df = parse_response(response)\n",
        "                total += len(df)\n",
        "                df.to_csv(csv_path, index=False)\n",
        "                print('Saving ' + csv_path + '...')\n",
        "    print('Number of articles collected: ' + str(total))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tvg9M5_FK-w",
        "outputId": "b7ee9aac-6bd9-494a-b30d-df067fa7f947"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Date range: ['2024', '1'] to ['2024', '2']\n"
          ]
        }
      ],
      "source": [
        "end = datetime.date(2024, 2, 1)\n",
        "start = datetime.date(2024, 1, 1)\n",
        "\n",
        "\n",
        "months = [x.split(' ') for x in pd.date_range(start, end, freq='MS').strftime(\"%Y %-m\").tolist()]\n",
        "\n",
        "\n",
        "get_data(months)\n",
        "\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "os.chdir(\"/content/headlines\") ## use Google Colab\n",
        "\n",
        "\n",
        "extension = 'csv'\n",
        "all_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n",
        "\n",
        "\n",
        "#combine in a single file\n",
        "combined_csv = pd.concat([pd.read_csv(f) for f in all_filenames ])\n",
        "#export to csv\n",
        "combined_csv.to_csv( \"combined_csv.csv\", index=False, encoding='utf-8-sig')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTvzQp1BHbGa"
      },
      "outputs": [],
      "source": [
        "pip install diffusers --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rkQREzuHnXx"
      },
      "outputs": [],
      "source": [
        "pip install invisible_watermark transformers accelerate safetensors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fL8AHav0Ju3V"
      },
      "outputs": [],
      "source": [
        "from diffusers import DiffusionPipeline\n",
        "import torch\n",
        "\n",
        "pipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, use_safetensors=True, variant=\"fp16\")\n",
        "pipe.enable_model_cpu_offload()\n",
        "\n",
        "# if using torch < 2.0\n",
        "# pipe.enable_xformers_memory_efficient_attention()\n",
        "\n",
        "prompt = \"An astronaut riding a green horse\"\n",
        "\n",
        "images = pipe(prompt=prompt).images[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qw1ycWCHKJ6Z",
        "outputId": "124e8640-63b9-4dff-adb0-9dd76c3acd4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: pydub, ffmpy, websockets, typing-extensions, tomlkit, shellingham, semantic-version, ruff, python-multipart, orjson, h11, colorama, annotated-types, aiofiles, uvicorn, starlette, pydantic-core, httpcore, pydantic, httpx, gradio-client, fastapi, gradio\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.10.14\n",
            "    Uninstalling pydantic-1.10.14:\n",
            "      Successfully uninstalled pydantic-1.10.14\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-23.2.1 annotated-types-0.6.0 colorama-0.4.6 fastapi-0.109.2 ffmpy-0.3.1 gradio-4.16.0 gradio-client-0.8.1 h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 orjson-3.9.13 pydantic-2.6.0 pydantic-core-2.16.1 pydub-0.25.1 python-multipart-0.0.7 ruff-0.2.0 semantic-version-2.10.0 shellingham-1.5.4 starlette-0.36.3 tomlkit-0.12.0 typing-extensions-4.9.0 uvicorn-0.27.0.post1 websockets-11.0.3\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZpDrK_wKIYY"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "gr.load(\"models/stabilityai/stable-diffusion-xl-base-1.0\").launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHAU3jYNLnAh"
      },
      "outputs": [],
      "source": [
        "from diffusers import DiffusionPipeline\n",
        "\n",
        "pipeline = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP1uaY10Q2lA5aNyf0C/tn9",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}